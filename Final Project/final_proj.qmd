---
title: "Data Analysis: Technology Use and Mental Health"
author: "Hailey Cantwell"
date: "04-25-2025"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(patchwork)
library(ggridges)
library(marginaleffects)
library(tidytext)
library(vip)
library(ordinal)
library(MASS)
library(pdp)

options(kable_styling_bootstrap_options = c("hover", "striped"))
theme_set(theme_bw(base_size = 14))
```

```{r}
data <- read_csv("mental_health_and_technology_usage_2024.csv") %>% 
  janitor::clean_names() %>%
  dplyr::select(-user_id) %>%
  mutate(
    stress_level = factor(stress_level, levels = c("Low","Medium","High"), ordered = TRUE),
    mental_health_status = factor(mental_health_status,
      levels = c("Poor","Fair","Good","Excellent"), ordered = TRUE
    )
  )


```

```{r}
set.seed(123)
data_splits <- initial_split(data, prop = 0.85, strata = social_media_usage_hours)
train      <- training(data_splits)
temp       <- testing(data_splits)

set.seed(456)
test_splits <- initial_split(temp, prop = 0.6)
validation  <- training(test_splits)
test        <- testing(test_splits)

# Create 5‐fold CV splits stratified on the outcome
set.seed(123)
cv_splits <- vfold_cv(train, v = 5, strata = mental_health_status)

```

# Statement of Purpose

The purpose of this analysis is to explore how daily technology usage, including screen time and social media engagement, impact mental health indicators such as stress levels, sleep quality, and overall mental wellness. This data set aims to provide insight that may guide researchers and assist is forming healthier technology habits to improve overall well-being.

# Executive Summary

The goal of this analysis was to assess whether variables of technology use, most notabley screen time and social media use, can reliably predict self-reported mental health status of individuals. In which I found a data set from [Kaggle](https://www.kaggle.com/datasets/waqi786/mental-health-and-technology-usage-dataset/data) to utilize. In which, I worked with the raw training data with the variables such as the ratio of social media to screen time, prior to training and tuning three different classifications models: random forest, ordinal logistic, and XGBoost under a 5-fold cross-validation frame.

Within this test set, the random forest displayed the highest accuracy at 27%, which only narrowly surpassed the other two models which reached \~24% accuracy. The predictive power of this set is rather modest, yet it is shown that screen time and social media engagement are the strongest predictors of mental health outcomes.

The following findings display that whilst technology usage patterns show some predictive powers, they only account for a sliver of the variance of mental health status responses. Further analyses of more broad datasets may provide varying results or incorporate additional psychological, social, or behavioral factors, whilst also exploring the varying outcomes of social media usage to better improve future predictions.

Technology usage continues to rise and become accessible at even younger ages, making it vital to understand the deeper impacts that it may have on mental health and other psychological aspects. Research shows that excessive amounts of screen time may predict higher levels of anxiety and depression, however results vary due to individual and environmental factors. Leading me to ask the question of how can measures of daily technology usage assist in predicting self-reported mental health statuses of adults?

# Introduction

Technology has become increasingly more accessible over the past fifteen years. In which there is a growing concern regarding the potential impacts it may have on mental health. Further research is vital for furthering intervention methods and forming public health guidelines regarding usage. There has been an abundance of research that links excessive technology usage to heightened levels of anxiety and depression, importantly this does vary depending on each person.

# Exploratory Data Analysis

Within this section, I will be utilizing the training set to further explore the potential relationships that exist between our predictors. In which the goal is to better understand which variables may influence mental health and further motivate future directions.

To begin I inspected the first few rows of the raw data to ensure variable names and the structure of the data. I did utilize janitor to make the labels easier to consume.

```{r}
head(data) %>%
  kable() %>%
  kable_styling()
```

Next I continued by examining the distribution of mental health status and stress levels to understand if any imbalances exist that may impact modeling.

```{r}
train %>%
  ggplot(aes(mental_health_status, fill = mental_health_status)) +
  geom_bar() +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Distribution of Mental Health Status", x = NULL, y = "Count")

train %>%
  ggplot(aes(stress_level, fill = stress_level)) +
  geom_bar() +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Distribution of Stress Levels", x = NULL, y = "Count")

```

We continue by exploring the age-based trends that may exist in stress levels and mental health status, in which it is seen that each level had an appropriate representation of each age.

```{r}
train %>%
  ggplot(aes(age, as.numeric(stress_level), color = stress_level)) +
  geom_jitter(width = 0.5, height = 0.1, alpha = 0.6) +
  geom_smooth(aes(group = 1), method = "loess", se = TRUE, color = "gray30") +
  scale_color_brewer(palette = "Set2") +
  scale_y_continuous(breaks = 1:3, labels = c("Low","Medium","High")) +
  labs(title = "Stress Level by Age", x = "Age", y = "Stress Level", color = NULL) +
  theme_minimal()

```

```{r}
train %>%
  ggplot(aes(age, as.numeric(mental_health_status), color = mental_health_status)) +
  geom_jitter(width = 0.5, height = 0.1, alpha = 0.6) +
  geom_smooth(aes(group = 1), method = "loess", se = TRUE, color = "gray30") +
  scale_color_brewer(palette = "Set2") +
  scale_y_continuous(breaks = 1:4, labels = c("Poor","Fair","Good","Excellent")) +
  labs(title = "Mental Health Status by Age", x = "Age", y = NULL, color = NULL) +
  theme_minimal()

```

I utilized a ridgeline density to see how social media usage may differ in regard to mental health status. There allows for us to see the distribution across each level of mental health status.

```{r}
train %>%
  ggplot(aes(social_media_usage_hours, mental_health_status, fill = mental_health_status)) +
  geom_density_ridges(alpha = 0.8, scale = 1.1, bandwidth = 0.5) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Social Media Usage by MH Status", x = "Hours/day", y = NULL, fill = NULL) +
  theme_ridges(grid = TRUE)


```

In the final piece of this data analysis I computed and visualized correlations among all of the numerical predictors. This heatmap will assist in better understanding the patterns that exist between variables.

```{r}
train %>%
  dplyr::select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs") %>%
  reshape2::melt() %>%
  ggplot(aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low="navy", mid="white", high="firebrick", midpoint=0) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title = "Correlation Matrix of Numeric Features")
```

# Model Construction

In this section, I depict how I transformed the variables and built three predictive models: random forest, ordinal logistic regression, and XGBoost. In which I used 5-fold cross-validation and hyperparameterer tuning. Each model was then finalized on the training set and evaluated based on the performance on the test data.

## Feature Engineering and Pre-processing

Prior to training any models I enhanced the predictors by creating a composite metric, normalized the data, and dummy-encoded categorical variables. I also created different composite metrics as well to look deeper at the patterns that may exist.

```{r}
recipe_class <- recipe(mental_health_status ~ ., data = train) %>%
  step_mutate(
    sm_to_screen     = social_media_usage_hours / screen_time_hours,
    screen_to_sleep  = screen_time_hours    / sleep_hours,
    gaming_to_screen = gaming_hours         / screen_time_hours,
    sm_x_age         = social_media_usage_hours * age
  ) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_poly(age, degree = 2) %>%
  step_dummy(all_nominal_predictors())

```

I continued to create a random forest model by utilizing the ranger function.

```{r}
rf_spec_tune <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf_tune <- workflow() %>%
  add_recipe(recipe_class) %>%
  add_model(rf_spec_tune)

```

## Model Re-sampling & Hyper-parameter Tuning

I generated a random tuning grid of 20, which preformed 5-fold CV to optimize accuracy of predictions.

```{r}
# parameter grid
rf_params <- hardhat::extract_parameter_set_dials(rf_spec_tune) %>%
  update(
    mtry  = mtry(range = c(2,10)),
    trees = trees(range = c(100,1000)),
    min_n = min_n(range = c(2,20))
  )
rf_grid <- grid_random(rf_params, size = 20)

# tuning
set.seed(123)
rf_tune_res <- tune_grid(
  rf_wf_tune,
  resamples = cv_splits,
  grid      = rf_grid,
  metrics   = metric_set(accuracy, kap, mn_log_loss),
  control   = control_grid(verbose = TRUE)
)

rf_tune_res %>% collect_metrics()
autoplot(rf_tune_res) + labs(title = "RF Tuning Results")

```

# Model Interpretation and Interence

To finalize the workflow I chose the best hyper-parameters, fit the full training set, and evaluated on the test set.

```{r}

# 1. (Re)fit your final workflow on the full training data
rf_final_wf <- finalize_workflow(rf_wf_tune, best_rf_params)
rf_final_fit <- fit(rf_final_wf, data = train)

# 2. Quick sanity check: make sure 'test' has the original raw columns
print("Test set columns:")
print(colnames(test))

# 3. Get predictions — the workflow will apply recipe_class (with all composites, dummies, etc.) under the hood
rf_test_probs <- predict(rf_final_fit, new_data = test, type = "prob")
rf_test_class <- predict(rf_final_fit, new_data = test, type = "class")

# 4. Combine predictions with the true outcomes
rf_test_preds <- bind_cols(rf_test_probs, rf_test_class, test)

# 5. Compute your metrics
rf_test_metrics <- rf_test_preds %>%
  metrics(truth = mental_health_status, estimate = .pred_class)
print(rf_test_metrics)

# 6. (Optional) Confusion matrix heatmap
rf_test_preds %>%
  conf_mat(truth = mental_health_status, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "RF Confusion Matrix on Test Set")

```

## Ordinal Logistic Regression

I continued to fitting the ordinal logisitc regression model by utilizing the same recipe without the use of the hyperparameter tuning. This was done to compare models and ensure the highest level of accuracy.

```{r}

# 1. Prep & bake both train and test through the same recipe
rec_prep    <- recipe_class %>% prep()
train_baked <- bake(rec_prep, new_data = train)
test_baked  <- bake(rec_prep, new_data = test)

# 2. Fit an ordinal logit with polr()
polr_fit <- polr(
  mental_health_status ~ .,
  data = train_baked,
  Hess = TRUE
)

# 3. Predict on the test set
polr_class <- predict(polr_fit, newdata = test_baked, type = "class")

# 4. Build a tibble of truth vs. predictions
polr_preds <- tibble(
  truth       = test_baked$mental_health_status,
  .pred_class = polr_class
)

# 5. Compute metrics
polr_metrics <- polr_preds %>%
  metrics(truth = truth, estimate = .pred_class)
print(polr_metrics)

# 6. Confusion matrix heatmap
polr_preds %>%
  conf_mat(truth = truth, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "Ordinal Logit (polr) Confusion Matrix")

```

The third model I generated was the XGBoost model, tuned over the same CV split, then plotted the tuning results to compare to the prior to models.

```{r}
## Gradient‐Boosted Tree Tuning

# 1. Define a boost_tree spec with tunable hyperparameters
boost_spec <- boost_tree(
  trees      = tune(),        # number of trees
  tree_depth = tune(),        # max depth
  learn_rate = tune()         # learning rate
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 2. Create a workflow re‐using your recipe
boost_wf <- workflow() %>%
  add_recipe(recipe_class) %>%
  add_model(boost_spec)

# 3. Build a random tuning grid
boost_params <- extract_parameter_set_dials(boost_spec) %>%
  update(
    trees      = trees(range = c(100, 1000)),
    tree_depth = tree_depth(range = c(3, 10)),
    learn_rate = learn_rate(range = c(0.01, 0.3))
  )
boost_grid <- grid_random(boost_params, size = 20)

# 4. Tune with your existing CV splits
set.seed(456)
boost_res <- tune_grid(
  boost_wf,
  resamples = cv_splits,
  grid      = boost_grid,
  metrics   = metric_set(accuracy, kap),
  control   = control_grid(verbose = TRUE)
)

# 5. View results
boost_res %>% collect_metrics() %>% filter(.metric == "accuracy")
autoplot(boost_res) + labs(title = "Boosted Tree Tuning")

```

I pulled the best hyperparameters, finalized the data, fit to the training set, and evaluated on the test set.

```{r}
# 1. Pull out the best hyperparameters by accuracy
best_boost_params <- boost_res %>% select_best(metric = "accuracy")

# 2. Finalize the workflow
boost_final_wf <- finalize_workflow(boost_wf, best_boost_params)

# 3. Fit on all of train
boost_final_fit <- fit(boost_final_wf, data = train)

# 4. Predict on test
boost_test_probs <- predict(boost_final_fit, new_data = test, type = "prob")
boost_test_class <- predict(boost_final_fit, new_data = test, type = "class")

boost_test_preds <- bind_cols(boost_test_probs, boost_test_class, test)

# 5. Compute test‐set metrics
boost_test_metrics <- boost_test_preds %>%
  metrics(truth = mental_health_status, estimate = .pred_class)
print(boost_test_metrics)

# 6. Confusion matrix
boost_test_preds %>%
  conf_mat(truth = mental_health_status, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "Boosted Tree Confusion Matrix")

```

We continue to extract the final ranger object by accuracy and plot the variable importance from the FRF.

```{r}
# 1. Pull out the RF’s best hyperparameters by accuracy
best_rf_params <- rf_tune_res %>% 
  select_best(metric = "accuracy")

# 2. Finalize your RF workflow
rf_final_wf <- finalize_workflow(rf_wf_tune, best_rf_params)

# 3. Fit the finalized RF on ALL of your training data
rf_final_fit <- fit(rf_final_wf, data = train)

# 4. Extract the underlying ranger model
final_rf_obj <- extract_fit_parsnip(rf_final_fit)$fit

# 5. Plot variable importance
vip(final_rf_obj) +
  labs(title = "Variable Importance: Final Random Forest")
```

I utilized the pdp package to assist in visualizing the marginal effects for the key variables/features.

```{r}


# 1. Prep & bake the recipe on your training data
rec_prep    <- recipe_class %>% prep()
train_baked <- bake(rec_prep, new_data = train)

# 2. Extract the raw ranger model
final_rf_obj <- extract_fit_parsnip(rf_final_fit)$fit

# 3. Compute the PDP, supplying train_baked explicitly
pdp_tech <- partial(
  object          = final_rf_obj,
  pred.var        = "technology_usage_hours",
  train           = train_baked,
  prob            = TRUE,
  grid.resolution = 25
)

# 4. Plot it
autoplot(pdp_tech) +
  labs(
    title = "Partial-Dependence: Technology Usage",
    x     = "Technology Usage (hrs/day)",
    y     = "Predicted Probability",
    colour = "MH Status"
  )

```

```{r}
# (reuse rec_prep, train_baked, final_rf_obj from before)

pdp_sleep <- partial(
  final_rf_obj,
  pred.var        = "sleep_hours",
  train           = train_baked,
  prob            = TRUE,
  grid.resolution = 25
)

autoplot(pdp_sleep) +
  labs(
    title  = "Partial-Dependence: Sleep Hours",
    x      = "Sleep Hours",
    y      = "Predicted Probability",
    colour = "MH Status"
  )


```

```{r}
pdp_ratio <- partial(
  final_rf_obj,
  pred.var        = "sm_to_screen",
  train           = train_baked,
  prob            = TRUE,
  grid.resolution = 25
)

autoplot(pdp_ratio) +
  labs(
    title  = "Partial-Dependence: SM / Screen Time",
    x      = "SM / Screen Time Ratio",
    y      = "Predicted Probability",
    colour = "MH Status"
  )

```

Here you can visualize the accuracy, kappa, and logloss for the three models.

```{r}
results_tbl <- tribble(
  ~Model,                  ~Accuracy, ~Kappa,   ~LogLoss,
  "Random Forest",         0.2696,    0.0261,   1.40,
  "Ordinal Logistic (polr)", 0.2379, -0.0293,  NA,
  "XGBoost",               0.2379,   -0.0166,  NA
)

results_tbl %>%
  kable(digits = c(NA, 4, 4, 2),
        caption = "Test-set Performance of All Models") %>%
  kable_styling(full_width = FALSE)


```

# Conclusion 

The random forest predictive model achieved the highest accuracy at \~27%, which was then followed by the other two models around \~24% accuracy. This displays an incredibly modest predictive model, which indicates that technology usage only only captures a sliver of mental health variances. Notably, screen time and social media usage were found to be the most influential predictors, whilst sleep and physical activity contributed much less.

For future work, having larger, more broad samples of data would be beneficial to create a more accurate mode. As well as integrating various psychosocial metrics to measure mood and social supports instead of self-report response, may assist in providing deeper insights overall.

# References 

waqi786. (2024). *Mental Health & Technology Usage Dataset* \[Data set\]. Kaggle. <https://www.kaggle.com/datasets/waqi786/mental-health-and-technology-usage-dataset/data>
