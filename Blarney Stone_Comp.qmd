---
title: "The Blarney Stone"
format: html
editor: visual
---

```{r}

library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggridges)
library(marginaleffects)

```

```{r}
data <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/blarney_data.csv")
comp <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/blarney_comp.csv")
```

```{r}
names(data) <- janitor::make_clean_names(names(data))

data <- data %>%
  mutate(
    transported = factor(kissed)
  )

set.seed(434)
data_splits <- initial_split(data, prop = 0.8, strata = kissed)

train <- training(data_splits)
test <- testing(data_splits)
```

```{r}
data %>%
  head() %>%
  kable()
```

```{r}
train %>%
  ggplot() + 
  geom_bar(aes(x = kissed)) + 
  labs(title = "Distribution of Transported Passengers",
       x = "Did you kiss the Blarney Stone?",
       y = "Count") + 
  theme_bw()
```

```{r}
pct_transported <- train %>%
  count(kissed) %>%
  ungroup() %>%
  mutate(pct = 100*n/sum(n)) %>%
  filter(kissed == "yes") %>%
  pull(pct)
```

```{r}
p1 <- train %>%
  ggplot() + 
  geom_bar(aes(x = weather)) + 
  labs(
    title = "Weather",
    x = "",
    y = "Count"
  ) + 
  coord_flip() + 
  theme_bw()

p2 <- train %>%
  ggplot() + 
  geom_bar(aes(x = nationality)) +
  labs(
    title = "Nationality",
    y = "Count",
    x = ""
  ) + theme_bw()

p3 <- train %>%
  ggplot() + 
  geom_bar(aes(x = kissed)) + 
  labs(
    title = "Kissed",
    x = "",
    y = "Count"
  ) + 
  coord_flip() + 
  theme_bw()

p4 <- train %>%
  ggplot() + 
  geom_bar(aes(x = filmed)) + 
  labs(
    title = "filmed",
    x = "",
    y = "Count"
  ) + 
  theme_bw()

(p1 + p2) / (p3 + p4)
```

```{r}
#| message: false
#| warning: false

train %>%
  ggplot() + 
  geom_histogram(aes(x = age), color = "black",
                 fill = "purple") + 
  labs(
    title = "Ages",
    x = "Age (Years)",
    y = ""
  ) + 
  theme_bw()
```

```{r}
#| message: false
#| warning: false

p1 <- train %>%
  ggplot() + 
  geom_density(aes(x = flexibility),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = flexibility, y = -0.005),
               fill = "purple",
               width = 0.002) + 
  labs(
    title = "flexibility",
    x = "Flexibility",
    y = ""
    ) + 
  theme_bw()

p2 <- train %>%
  ggplot() + 
  geom_density(aes(x = flexibility),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = flexibility, y = -0.05),
               fill = "purple",
               width = 0.02) + 
  labs(
    title = "How flexible?",
    x = "flexibility",
    y = ""
    ) + 
  scale_x_log10() + 
  theme_bw()

p1 + p2
```

```{r}
names(data) <- janitor::make_clean_names(names(data))
names(comp) <- janitor::make_clean_names(names(comp))
set.seed(304)
split <- initial_split(data, prop = 0.8)
train <- training(split)
test <- testing(split)
unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
train_folds <- vfold_cv(train, v = 10)
```

```{r}
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
dt_rec <- recipe(kissed ~ ., data = train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_impute_median(all_numeric_predictors())
dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)
```

```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)
tictoc::tic()
dt_cv_results <- dt_wf %>%
  fit_resamples(
    resamples = train_folds,
    metrics = metric_set(accuracy, mn_log_loss)
  )
tictoc::toc()
doParallel::stopImplicitCluster()
unregister()
dt_cv_results %>%
  collect_metrics()
```

```{r}
dt_tune_spec <- decision_tree(min_n = tune(), tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")
dt_tune_rec <- recipe(kissed ~ ., data = train) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_impute_median(all_numeric_predictors())
dt_tune_wf <- workflow() %>%
  add_model(dt_tune_spec) %>%
  add_recipe(dt_tune_rec)
```

```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)
tictoc::tic()
dt_tune_cv_results <- dt_tune_wf %>%
  tune_grid(
    resamples = train_folds,
    grid = 12,
    metrics = metric_set(mn_log_loss),
    initial = 5,
    control = control_bayes(parallel_over = "everything")
  )
tictoc::toc()
doParallel::stopImplicitCluster()
unregister()
dt_tune_cv_results %>%
  collect_metrics()
```

```{r}
dt_tune_cv_results %>%
  show_best(n = 10)
```

```{r}
dt_best_params <- dt_tune_cv_results %>%
  select_best(metric = "mn_log_loss")
dt_best_wf <- dt_tune_wf %>%
  finalize_workflow(dt_best_params)
dt_best_fit <- dt_best_wf %>%
  fit(train)
```

```{r}
Chloe_Hailey_submission <- dt_best_fit %>%
  augment(comp) %>%
  rename(kissed = .pred_yes) %>%
  select(id, kissed)
write.csv(Chloe_Hailey_submission, "Chloe_Hailey_submission.csv", row.names = FALSE)
```
